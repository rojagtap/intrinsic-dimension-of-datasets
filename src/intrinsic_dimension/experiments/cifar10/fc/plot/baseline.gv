digraph {
	graph [size="12,12"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	133584270128400 [label="
 ()" fillcolor=darkolivegreen1]
	133584267311184 -> 133584560179616 [dir=none]
	133584560179616 [label="self
 (64, 10)" fillcolor=orange]
	133584267311184 -> 133584560232128 [dir=none]
	133584560232128 [label="target
 (64)" fillcolor=orange]
	133584267311184 -> 133584560177456 [dir=none]
	133584560177456 [label="total_weight
 ()" fillcolor=orange]
	133584267311184 [label="NllLossBackward0
----------------------------------
ignore_index: 18446744073709551516
reduction   :                    1
self        :       [saved tensor]
target      :       [saved tensor]
total_weight:       [saved tensor]
weight      :                 None"]
	133584267311232 -> 133584267311184
	133584267311232 -> 133584560190016 [dir=none]
	133584560190016 [label="result
 (64, 10)" fillcolor=orange]
	133584267311232 [label="LogSoftmaxBackward0
----------------------
dim   :              1
result: [saved tensor]"]
	133584267312528 -> 133584267311232
	133584267312528 -> 133584267056432 [dir=none]
	133584267056432 [label="mat1
 (64, 200)" fillcolor=orange]
	133584267312528 -> 133584560179296 [dir=none]
	133584560179296 [label="mat2
 (200, 10)" fillcolor=orange]
	133584267312528 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (64, 200)
mat1_sym_strides:       (200, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :      (200, 10)
mat2_sym_strides:       (1, 200)"]
	133584267311280 -> 133584267312528
	133584560231488 [label="linear_relu_stack.linear_output.bias
 (10)" fillcolor=lightblue]
	133584560231488 -> 133584267311280
	133584267311280 [label=AccumulateGrad]
	133584267310128 -> 133584267312528
	133584267310128 -> 133584266641712 [dir=none]
	133584266641712 [label="result
 (64, 200)" fillcolor=orange]
	133584267310128 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	133584267312384 -> 133584267310128
	133584267312384 -> 133584560231328 [dir=none]
	133584560231328 [label="mat1
 (64, 200)" fillcolor=orange]
	133584267312384 -> 133584560179776 [dir=none]
	133584560179776 [label="mat2
 (200, 200)" fillcolor=orange]
	133584267312384 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (64, 200)
mat1_sym_strides:       (200, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (200, 200)
mat2_sym_strides:       (1, 200)"]
	133584267312816 -> 133584267312384
	133584560231648 [label="linear_relu_stack.linear_0.bias
 (200)" fillcolor=lightblue]
	133584560231648 -> 133584267312816
	133584267312816 [label=AccumulateGrad]
	133584267312768 -> 133584267312384
	133584267312768 -> 133584560188576 [dir=none]
	133584560188576 [label="result
 (64, 200)" fillcolor=orange]
	133584267312768 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	133584267309840 -> 133584267312768
	133584267309840 -> 133584268790336 [dir=none]
	133584268790336 [label="mat1
 (64, 3072)" fillcolor=orange]
	133584267309840 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (64, 3072)
mat1_sym_strides:             ()
mat2            :           None
mat2_sym_sizes  :    (3072, 200)
mat2_sym_strides:      (1, 3072)"]
	133584267312960 -> 133584267309840
	133584560231968 [label="linear_relu_stack.linear_input.bias
 (200)" fillcolor=lightblue]
	133584560231968 -> 133584267312960
	133584267312960 [label=AccumulateGrad]
	133584267312912 -> 133584267309840
	133584267312912 [label=TBackward0]
	133584267313008 -> 133584267312912
	133584268110608 [label="linear_relu_stack.linear_input.weight
 (200, 3072)" fillcolor=lightblue]
	133584268110608 -> 133584267313008
	133584267313008 [label=AccumulateGrad]
	133584267312336 -> 133584267312384
	133584267312336 [label=TBackward0]
	133584267313056 -> 133584267312336
	133584560231888 [label="linear_relu_stack.linear_0.weight
 (200, 200)" fillcolor=lightblue]
	133584560231888 -> 133584267313056
	133584267313056 [label=AccumulateGrad]
	133584267311472 -> 133584267312528
	133584267311472 [label=TBackward0]
	133584267310080 -> 133584267311472
	133584560231568 [label="linear_relu_stack.linear_output.weight
 (10, 200)" fillcolor=lightblue]
	133584560231568 -> 133584267310080
	133584267310080 [label=AccumulateGrad]
	133584267311184 -> 133584270128400
}
